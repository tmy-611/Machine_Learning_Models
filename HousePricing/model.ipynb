{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "From this dataset, I build a Machine Learning model in Spark and Java to predict the house price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I follow each steps below:\n",
    "\n",
    "1. Foundation\n",
    "- Set up environment\n",
    "- Initial Spark Context\n",
    "2. Explore and Preprocessing data\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Feature Selecting\n",
    "- Missing Values Handling\n",
    "- Feature Engineering: StringIndexing, OneHotEncoding, VectorAssembling methods\n",
    "3. Train-Test Split\n",
    "4. Choose Model for Training and Prediction\n",
    "5. Evaluate The Model\n",
    "6. Plot the Prediction value and Actual value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"House Pricing Prediction\") \\\n",
    "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "hp = spark.read.csv('housprice.csv', header=True, inferSchema=True)\n",
    "hp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "hp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The schema seems does not match with the value. Let's redefine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "hp = hp.withColumn(\"LotFrontage\", col(\"LotFrontage\").cast(IntegerType()))\n",
    "hp = hp.withColumn(\"MasVnrArea\", col(\"MasVnrArea\").cast(IntegerType()))\n",
    "hp = hp.withColumn(\"GarageYrBlt\", col(\"GarageYrBlt\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "hp.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selecting\n",
    "\n",
    "- Select those features that have strong impact on the SellPrice value. (e.g. if it increase --> the price increase)\n",
    "- Ignore those features that have imbalanced distribution between the values.\n",
    "- Ignore those features that have too much NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['LotFrontage', 'LotArea', 'Neighborhood', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea','ExterQual', 'ExterCond', 'BsmtQual', 'Electrical', '1stFlrSF', 'LowQualFinSF', 'BedroomAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'GarageArea', 'PavedDrive', 'WoodDeckSF', 'SaleCondition' , 'SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe based on new selected feeatures\n",
    "new_hp = hp.select(selected_features)\n",
    "new_hp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Missing Values Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical/numerical features\n",
    "numeric_cols = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', '1stFlrSF', 'LowQualFinSF', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageArea', 'WoodDeckSF']\n",
    "categorical_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'BsmtQual', 'Electrical', 'KitchenQual', 'PavedDrive', 'SaleCondition' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing values\n",
    "from pyspark.sql.functions import col, when, isnan, count\n",
    "\n",
    "missing_values = new_hp.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in new_hp.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing value\n",
    "from pyspark.sql.functions import col, when, max\n",
    "\n",
    "# With numerical attributes\n",
    "# Convert 'NA' strings to null values for all numeric attributes\n",
    "new_hp = new_hp.select([when(col(c) == 'NA', None).otherwise(col(c)).alias(c) for c in new_hp.columns])\n",
    "\n",
    "# Fill null values with 0\n",
    "new_hp = new_hp.fillna(0)\n",
    "\n",
    "# With categorical attributes\n",
    "# Fill null values in categorical attributes with the most frequent value\n",
    "for column_name in categorical_cols:\n",
    "    new_hp = new_hp.na.fill(new_hp.groupBy().agg(max(col(column_name))).collect()[0][0], subset=[column_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are any remaining missing values\n",
    "missing_values = new_hp.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in new_hp.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering: StringIndexing, OneHotEncoding, VectorAssembling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=\"indexed_\"+column) for column in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=\"indexed_\"+column, outputCol=\"encoded_\"+column) for column in categorical_cols]\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# fit and transform the pipeline\n",
    "encoded_hp = pipeline.fit(new_hp).transform(new_hp)\n",
    "encoded_hp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select indexed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern \n",
    "pattern = re.compile(r'^indexed_')\n",
    "\n",
    "# Subtract the set of columns that match the pattern\n",
    "indexed_columns = [col for col in encoded_hp.columns if pattern.match(col)]\n",
    "indexed_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select encoded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern \n",
    "pattern = re.compile(r'^encoded_')\n",
    "\n",
    "# Subtract the set of columns that match the pattern\n",
    "encoded_columns = [col for col in encoded_hp.columns if pattern.match(col)]\n",
    "encoded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembling\n",
    "assembler = VectorAssembler(inputCols=numeric_cols + encoded_columns, outputCol=\"features\")\n",
    "hp2 = assembler.transform(encoded_hp)\n",
    "hp2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the dataframe for easy use\n",
    "hp2.drop(*indexed_columns, *encoded_columns,*categorical_cols,*numeric_cols)\n",
    "hp2 = hp2.select('features','SalePrice')\n",
    "hp2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split data into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = hp2.randomSplit([0.8, 0.2], seed=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use Random Forest model to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the RandomForest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"SalePrice\", seed=56)\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using RMSE and R-squared\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"SalePrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"SalePrice\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (RÂ²):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plot the prediction value and actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Collect the predictions and actual values\n",
    "predictions_df = predictions.select(\"prediction\", \"SalePrice\").toPandas()\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(predictions_df[\"SalePrice\"], predictions_df[\"prediction\"], alpha=0.5)\n",
    "plt.plot([predictions_df[\"SalePrice\"].min(), predictions_df[\"SalePrice\"].max()],\n",
    "         [predictions_df[\"SalePrice\"].min(), predictions_df[\"SalePrice\"].max()],\n",
    "         color='red', lw=2)\n",
    "plt.xlabel(\"Actual SalePrice\")\n",
    "plt.ylabel(\"Predicted SalePrice\")\n",
    "plt.title(\"Actual vs Predicted SalePrice\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
